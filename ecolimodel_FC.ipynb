{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_LeXKMKVPDA"
      },
      "source": [
        "## import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5_csnzpWNfw"
      },
      "source": [
        "# Model Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-_o9rBzXZI3"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "# Import for read data function\n",
        "from scipy import io\n",
        "from scipy.io import loadmat\n",
        "from collections import OrderedDict\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlMgw2PHUumw"
      },
      "source": [
        "## model structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNAlTKwRyNub"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self,numNeurons=16):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # self.conv1 = nn.Conv2d(1,3,3,stride=2,padding=0)\n",
        "        # self.bn2d1 = nn.BatchNorm2d(3)\n",
        "\n",
        "        self.fc1 = nn.Linear(160, numNeurons*8).double()\n",
        "        self.fc2 = nn.Linear(numNeurons*8, numNeurons*8).double()\n",
        "        self.fc3 = nn.Linear(numNeurons*8, numNeurons*4).double()\n",
        "        self.fc4 = nn.Linear(numNeurons*4, numNeurons*2).double()\n",
        "        self.fc5 = nn.Linear(numNeurons*2, 4).double()\n",
        "        self.dropout=nn.Dropout(p=0.25)\n",
        "\n",
        "        # self.fc1 = nn.Linear(164, numNeurons*2)\n",
        "        # # self.fc2 = nn.Linear(numNeurons*8, numNeurons*4)\n",
        "        # self.fc3 = nn.Linear(numNeurons*2, numNeurons)\n",
        "        # self.fc4 = nn.Linear(numNeurons, 6)\n",
        "        # self.dropout=nn.Dropout(p=0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.conv1(x)\n",
        "        # x=self.bn2d1(x)\n",
        "        # x=F.relu6(x)\n",
        "\n",
        "        # x=x.view(x.size()[0],-1) # flatten the activation\n",
        "        x=self.fc1(x)\n",
        "        x=F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x=self.fc2(x)\n",
        "        x=F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x=self.fc3(x)\n",
        "        x=F.relu(x)\n",
        "\n",
        "        x=self.fc4(x)\n",
        "        x=F.relu(x)\n",
        "\n",
        "        x=self.fc5(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OIgPlBEU1QM"
      },
      "source": [
        "## connect to dirve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imPYUQnXVfbQ",
        "outputId": "f61a1ec7-8d8f-48f9-de60-cd25a2089dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UFLbzRwU_fQ"
      },
      "source": [
        "## set GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4ofHod4VaxH"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbR7gl_hVEzC"
      },
      "source": [
        "## set hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3MTQuZKVi4X"
      },
      "outputs": [],
      "source": [
        "epochs = 200\n",
        "lr = 0.001\n",
        "lossFunc=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpXHCLzNdqmw"
      },
      "outputs": [],
      "source": [
        "# remove all files\n",
        "# os.chdir('/content')\n",
        "# !ls\n",
        "# !rm -rf train\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTKKjikgWIsJ"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_XCQyNWHbQl"
      },
      "source": [
        "## load labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j792xdfTDVfF",
        "outputId": "242fcb01-d471-421a-e87c-0fcf4b9a2b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 5 5 ... 0 0 0]\n",
            "6875\n"
          ]
        }
      ],
      "source": [
        "path = \"/content/\"\n",
        "os.chdir(path)\n",
        "excel_file_path = 'drive/MyDrive/label.xlsx'\n",
        "\n",
        "df = pd.read_excel(excel_file_path)\n",
        "label_row = df.iloc[:6876, 0]\n",
        "labels = label_row.to_numpy()\n",
        "\n",
        "labels = labels - 1\n",
        "\n",
        "print(labels)\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT4WDGoVHhSa"
      },
      "source": [
        "## load .mat file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-W0nncGXsJf",
        "outputId": "d9356b9e-d4e1-4837-d311-5d535c92a45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['01 (1~45).mat', '02 (46~88).mat', '03 (89~132).mat', '04 (133~189).mat', '05 (190~241).mat', '06 (242~277).mat', '07 (278~308).mat', '08 (309~333).mat', '09 (334~364).mat', '10 (365~404).mat', '11 (405~435).mat', '12 (436~475).mat', '13 (476~509).mat', '14 (510~551).mat', '15 (552~587).mat', '16 (588~625).mat', '17 (626~670).mat', '18 (671~710).mat', '19 (711~740).mat', '20 (741~774).mat', '21 (775~824).mat', '22 (825~863).mat', '23 (864~906).mat', '24 (907~948).mat', '25 (949~1000).mat', '26 (1001~1033).mat', '27 (1034~1058).mat', '28 (1059~1122).mat', '29 (1123~1205).mat', '30 (1206~1250).mat', '31 (1251~1293).mat', '32 (1294~1330).mat', '33 (1331~1372).mat', '34 (1373~1433).mat', '35 (1434~1468).mat', '36 (1469~1516).mat', '37 (1517~1556).mat', '38 (1557~1594).mat', '39 (1595~1626).mat', '40 (1627~1654).mat', '41 (1655~1690).mat', '42 (1691~1730).mat', '43 (1731~1765).mat', '44 (1766~1800).mat', '45 (1801~1831).mat', '46 (1832~1853).mat', '47 (1854~1876).mat', '48 (1877~1901).mat', '49 (1902~1936).mat', '50 (1937~1972).mat', '51 (1973~2000).mat', '52 (2001-2014).mat', '53 (2015-2034).mat', '54 (2035-2067).mat', '55 (2068-2090).mat', '56 (2091-2130).mat', '57 (2131-2156).mat', '58 (2157-2182).mat', '59 (2183-2194).mat', '60 (2195-2216).mat', '61 (2217-2239).mat', '62 (2240-2267).mat', '63 (2268-2299).mat', '64 (2300-2329).mat', '65 (2330-2362).mat', '66 (2363-2388).mat', '67 (2389-2412).mat', '68 (2413-2434).mat', '69 (2435-2463).mat', '70 (2464-2483).mat', '71 (2484-2532).mat', '72 (2533-2555).mat', '73 (2556-2574).mat', '74 (2575-2603).mat', '75 (2604-2624).mat', '76 (2625-2658).mat', '77 (2659-2689).mat', '78 (2690-2739).mat', '79 (2740-2782).mat', '80 (2783-2834).mat', '81 (2835-2879).mat', '82 (2880-2934).mat', '83 (2935-2974).mat', '84 (2975-3000).mat', '85 (3001~3093).mat', '86 (3094~3164).mat', '87 (3165~3216).mat', '88 (3217~3266).mat', '89 (3267~3323).mat', '90 (3324~3365).mat', '91 (3366~3437).mat', '92 (3438~3514).mat', '93 (3515~3574).mat', '94 (3575~3635).mat', '95 (3636~3698).mat', '96 (3699~3775).mat', '97 (3776~3831).mat', '98 (3832~3888).mat', '99 (3889~3929).mat', '100 (3930~3977).mat', '101 (3978~4031).mat', '102 (4032~4100).mat', '103 (4101~4157).mat', '104 (4158~4230).mat', '105 (4231~4290).mat', '106 (4291~4318).mat', '107 (4319~4370).mat', '108 (4371-4380).mat', '109 (4381-4411).mat', '110 (4412-4424).mat', '111 (4425-4447).mat', '112 (4448-4464).mat', '113 (4465-4476).mat', '114 (4477-4491).mat', '115 (4492-4515).mat', '116 (4516-4521).mat', '117 (4522-4534).mat', '118 (4535-4541).mat', '119 (4542-4553).mat', '120 (4554-4565).mat', '121 (4566-4575).mat', '122 (4576-4582).mat', '123 (4583-4595).mat', '124 (4596-4602).mat', '125 (4603-4609).mat', '126 (4610-4614).mat', '127 (4615-4619).mat', '128 (4620-4622).mat', '129 (4623).mat', '130 (4624-4625).mat', '131 (4626-4628).mat', '132 (4629-4630).mat', '133 (4631-4634).mat', '134 (4635-4643).mat', '135 (4644-4650).mat', '136 (4651-4657).mat', '137 (4658-4664).mat', '138 (4665-4668).mat', '139 (4669-4676).mat', '141 (4677~4690).mat', '142 (4691~4704).mat', '143 (4705~4736).mat', '144 (4737~4772).mat', '145 (4773-4808).mat', '146 (4809-4832).mat', '147 (4833-4855).mat', '148 (4856-4870).mat', '149 (4871-4894).mat', '150 (4895-4912).mat', '151 (4913-4928).mat', '152 (4929~4955).mat', '153 (4956~4988).mat', '154 (4989~5019).mat', '155 (5020~5053).mat', '156 (5054~5083).mat', '157 (5084~5103).mat', '158 (5104~5128).mat', '159 (5129-5147).mat', '160 (5148-5178).mat', '161 (5179-5209).mat', '162 (5210-5244).mat', '163 (5245-5277).mat', '164 (5278-5321).mat', '165 (5322-5354).mat', '166 (5355-5387).mat', '167 (5388-5405).mat', '168 (5406-5416).mat', '169 (5417-5434).mat', '170 (5435-5450).mat', '171 (5451-5467).mat', '172 (5468-5482).mat', '173 (5483-5502).mat', '174 (5503-5524).mat', '175 (5525-5543).mat', '176 (5544-5560).mat', '177 (5561-5583).mat', '178 (5584-5611).mat', '179 (5612-5627).mat', '180 (5628-5647).mat', '181 (5648-5675).mat', '182 (5676-5700).mat', '183 (5701-5720).mat', '184 (5721-5733).mat', '185 (5734-5748).mat', '186 (5749-5772).mat', '187 (5773-5788).mat', '188 (5789-5805).mat', '189 (5806-5818).mat', '190 (5819-5820).mat', '191 (5821-5822).mat', '192 (5823-5824).mat', '193 (5825-5826).mat', '194 (5827-5830).mat', '195 (5831).mat', '196 (5832).mat', '197 (5833).mat', '198 (5834).mat', '199 (5835-5836).mat', '200 (5837-5838).mat', '201 (5839).mat', '202 (5840).mat', '203 (5841-5942).mat', '204 (5843).mat', '205 (5844-5845).mat', '206 (5846-5847).mat', '207 (5848-5854).mat', '208 (5855-5862).mat', '209 (5863-5864).mat', '210 (5865-5873).mat', '211 (5874-5878).mat', '212 (5879-5885).mat', '213 (5886-5891).mat', '214 (5892-5895).mat', '215 (5896-5904).mat', '216 (5905-5911).mat', '217 (5912-5918).mat', '218 (5919-5923).mat', '219 (5924-5929).mat', '220 (5930-5942).mat', '221 (5943-5953).mat', '222 (5954-5961).mat', '223 (5962-5970).mat', '224 (5971-5977).mat', '225 (5978-5984).mat', '226 (5985-5994).mat', '227 (5995-6003).mat', '228 (6004-6019).mat', '229 (6020-6025).mat', '230 (6026-6033).mat', '231 (6034-6038).mat', '232 (6039-6057).mat', '233 (6058-6065).mat', '234 (6066-6075).mat', '235 (6076-6092).mat', '236 (6093-6110).mat', '237 (6111-6121).mat', '238 (6122-6140).mat', '239 (6141-6146).mat', '240 (6147-6153).mat', '241 (6154-6159).mat', '242 (6160-6163).mat', '243 (6164-6168).mat', '244 (6169-6177).mat', '245 (6178-6188).mat', '246 (6189-6191).mat', '247 (6192-6193).mat', '248 (6194-6199).mat', '249 (6200-6205).mat', '250 (6206-6208).mat', '251 (6209-6215).mat', '252 (6216-6218).mat', '253 (6219-6222).mat', '254 (6223-6232).mat', '255 (6233-6235).mat', '256 (6236-6241).mat', '257 (6242-6243).mat', '258 (6244-6248).mat', '259 (6249-6263).mat', '260 (6264-6280).mat', '261 (6281-6305).mat', '262 (6306-6335).mat', '263 (6336-6359).mat', '264 (6360-6372).mat', '265 (6373-6389).mat', '266 (6390-6403).mat', '267 (6404-6419).mat', '268 (6420-6429).mat', '269 (6430-6443).mat', '270 (6444-6449).mat', '271 (6450-6456).mat', '272 (6457-6471).mat', '273 (6472-6481).mat', '274 (6482-6498).mat', '275 (6499-6522).mat', '276 (6523-6542).mat', '277 (6543-6560).mat', '278 (6561-6575).mat', '279 (6576-6592).mat', '280 (6593-6609).mat', '281 (6610-6628).mat', '282 (6629-6634).mat', '283 (6635-6650).mat', '284 (6651-6675).mat', '285 (6676~6679).mat', '286 (6680).mat', '287 (6681~6687).mat', '288 (6688~6692).mat', '289 (6693~6696).mat', '290 (6697~6699).mat', '291 (6700~6701).mat', '292 (6702~6707).mat', '293 (6708~6716).mat', '294 (6717~6721).mat', '295 (6722~6730).mat', '296 (6731~6734).mat', '297 (6735~6739).mat', '298 (6740~6741).mat', '299 (6742~6745).mat', '300 (6746~6750).mat', '301 (6751~6752).mat', '302 (6753~6760).mat', '303 (6761~6773).mat', '304 (6774~6775).mat', '305 (6776-6778).mat', '306 (6779-6783).mat', '307 (6784-6785).mat', '308 (6786-6793).mat', '309 (6794-6800).mat', '310 (6801-6806).mat', '311 (6807-6818).mat', '312 (6819-6824).mat', '313 (6825~6827).mat', '314 (6828~6845).mat', '315 (6846~6860).mat', '316 (6861~6875).mat']\n"
          ]
        }
      ],
      "source": [
        "path = \"/content/drive/MyDrive/train/\"\n",
        "\n",
        "os.chdir(path)\n",
        "files = os.listdir(path)\n",
        "files = [ff for ff in files if '.mat' in ff]\n",
        "\n",
        "def extract_number(filename):\n",
        "    return int(filename.split()[0])\n",
        "\n",
        "files = sorted(files, key=extract_number)\n",
        "\n",
        "print(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djy0wYziSW_M",
        "outputId": "cce4bba3-56ab-426c-9f79-0bff93104b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float32\n"
          ]
        }
      ],
      "source": [
        "Data = []\n",
        "\n",
        "for i in files:\n",
        "  f = loadmat(i)\n",
        "  data = f['cellList']['meshData']\n",
        "  cells_file = data[0][0][0][0][0]\n",
        "  # print(len(cells_file))\n",
        "  for j in range (len(cells_file)):\n",
        "    Base = cells_file[j][0][0]\n",
        "    Data.append(Base['signal1'])\n",
        "    Data.append(Base['signal2'])\n",
        "\n",
        "# print(len(Data))\n",
        "# print(Data[0])\n",
        "# print(Data[1])\n",
        "print(Data[0].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojGyIdRu5Hwp"
      },
      "source": [
        "## data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7MYed8NVfT6"
      },
      "source": [
        "expand length to 164"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1e4RDmiP6Ik"
      },
      "outputs": [],
      "source": [
        "def extrapo(s, extra_length):\n",
        "  desired_length = 80\n",
        "  new_vector = np.zeros(desired_length)\n",
        "\n",
        "  cnt = 0\n",
        "  for i in range(len(s)-1):\n",
        "    cnt = i\n",
        "    if i == extra_length:\n",
        "      break\n",
        "    new_vector[2 * i] = s[i]\n",
        "    new_vector[2 * i + 1] = (s[i] + s[i+1]) / 2\n",
        "\n",
        "  cnt_s = cnt\n",
        "  cnt = cnt * 2\n",
        "  nokori_len = len(s) - cnt_s\n",
        "  for i in range(nokori_len):\n",
        "    new_vector[cnt] = s[cnt_s]\n",
        "    cnt += 1\n",
        "    cnt_s += 1\n",
        "\n",
        "  for i in range(desired_length-cnt):\n",
        "    new_vector[cnt+i] = s[cnt_s-1]\n",
        "\n",
        "  return new_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooZQ6L1i1MlS"
      },
      "outputs": [],
      "source": [
        "def interpo(s):\n",
        "  desired_length = 80\n",
        "\n",
        "  remove_length = len(s) - desired_length\n",
        "  indices_to_remove = random.sample(range(len(s)), remove_length)\n",
        "\n",
        "  new_vector = [value for index, value in enumerate(s) if index not in indices_to_remove]\n",
        "\n",
        "  # print(len(new_vector))\n",
        "  for i in range(len(new_vector)):\n",
        "    new_vector[i] = new_vector[i]*2 / 2\n",
        "\n",
        "  return new_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "befUuhbPg4zX"
      },
      "source": [
        "concate & normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y800JIbS4_ua",
        "outputId": "e366a081-ba79-446f-d1d0-e0c7024a6c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6875\n",
            "[241, 3091, 5587, 6432]\n",
            "float64\n"
          ]
        }
      ],
      "source": [
        "modified_Data = []\n",
        "additional_Data = []\n",
        "indexes = []\n",
        "\n",
        "for i in range(int(len(Data)/2)):\n",
        "  temp1 = [element for sublist in Data[i*2] for element in sublist]\n",
        "  temp1 = np.array(temp1)\n",
        "  # normalization\n",
        "  if max(temp1) != 0:\n",
        "    temp1 = 255*temp1/max(temp1)\n",
        "  if len(temp1) < 3:\n",
        "    indexes.append(i)\n",
        "  if len(temp1) < 80:\n",
        "    temp1 = extrapo(temp1, (80-len(temp1)))\n",
        "  elif len(temp1) >= 80:\n",
        "    temp1 = interpo(temp1)\n",
        "  temp3 = np.flip(temp1)\n",
        "\n",
        "  temp2 = [element for sublist in Data[i*2+1] for element in sublist]\n",
        "  temp2 = np.array(temp2)\n",
        "  # normlization\n",
        "  if max(temp2) != 0:\n",
        "    temp2 = 255*temp2/max(temp2)\n",
        "  if len(temp2) < 80:\n",
        "    temp2 = extrapo(temp2, (80-len(temp2)))\n",
        "  elif len(temp2) >= 80:\n",
        "    temp2 = interpo(temp2)\n",
        "  temp4 = np.flip(temp2)\n",
        "\n",
        "  temp5 = np.concatenate((temp1, temp2), axis = 0)\n",
        "  modified_Data.append(temp5)\n",
        "\n",
        "  temp6 = np.concatenate((temp3, temp4), axis = 0)\n",
        "  additional_Data.append(temp6)\n",
        "\n",
        "# print(modified_Data[4])\n",
        "print(len(modified_Data))\n",
        "print(indexes)\n",
        "print(modified_Data[0].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK5CTlsnVugz"
      },
      "source": [
        "remove some bad data\n",
        "- length < 3\n",
        "- label = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiyriZAg_S-i",
        "outputId": "39f27c30-64b1-4f7f-cfb8-1e735dbd82d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "6871\n",
            "6871\n",
            "[241, 3091, 5587, 6432]\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(modified_Data)):\n",
        "  if labels[i] == 6:\n",
        "    indexes.append(i)\n",
        "\n",
        "pruned_modified_Data = []\n",
        "modified_additional_Data = []\n",
        "for i in range(len(modified_Data)):\n",
        "  if i not in indexes:\n",
        "    pruned_modified_Data.append(modified_Data[i])\n",
        "for i in range(len(additional_Data)):\n",
        "  if i not in indexes:\n",
        "    modified_additional_Data.append(additional_Data[i])\n",
        "pruned_labels = np.delete(labels, indexes)\n",
        "# temp = np.copy(pruned_labels)\n",
        "# pruned_labels = np.concatenate((pruned_labels, temp), axis=0)\n",
        "\n",
        "print(len(indexes))\n",
        "print(len(pruned_modified_Data))\n",
        "print(len(pruned_labels))\n",
        "print(indexes)\n",
        "# print(pruned_labels)\n",
        "# print(pruned_modified_Data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuZVidoZjrOw",
        "outputId": "8421e905-87ce-4b1b-f241-8e088354d74a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6871\n",
            "6871\n"
          ]
        }
      ],
      "source": [
        "combined_data = list(zip(pruned_modified_Data, modified_additional_Data, pruned_labels))\n",
        "np.random.shuffle(combined_data)\n",
        "shuffled_data, shuffled_additional_data, shuffled_labels = zip(*combined_data)\n",
        "\n",
        "print(len(shuffled_data))\n",
        "# print(shuffled_data[0])\n",
        "# print(shuffled_data[1])\n",
        "# print(Data[0].dtype)\n",
        "print(len(shuffled_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2St0kjjQWCim"
      },
      "source": [
        "## build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfGc3HxJZ-ig"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1pZCz6uWJlt"
      },
      "source": [
        "## partitioning dataset\n",
        "\n",
        "size of each set:\n",
        "- training set: 0.8\n",
        "- validation set: 0.1\n",
        "- testing set: 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQWotdL9l65M",
        "outputId": "d9639304-1776-4713-9d9b-7cf664ba42ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5496\n",
            "5496\n"
          ]
        }
      ],
      "source": [
        "total_data = len(shuffled_data)\n",
        "train_size = int(0.8 * total_data)\n",
        "val_size = int(0.1 * total_data)\n",
        "test_size = total_data - train_size - val_size\n",
        "\n",
        "training_data = shuffled_data[:train_size]\n",
        "training_label = shuffled_labels[:train_size]\n",
        "val_data = shuffled_data[train_size:train_size+val_size]\n",
        "val_label = shuffled_labels[train_size:train_size+val_size]\n",
        "test_data = shuffled_data[train_size+val_size:]\n",
        "test_label = shuffled_labels[train_size+val_size:]\n",
        "\n",
        "# dataset = MyDataset(pruned_modified_Data, pruned_labels)\n",
        "\n",
        "# train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# print(test_data[0].dtype)\n",
        "# print(len(train_set))\n",
        "# print(train_set.indices)\n",
        "print(len(training_data))\n",
        "print(len(training_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJsLoua5ht-Z"
      },
      "source": [
        "Data augmentaion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzlXfvROhr3N",
        "outputId": "787c1ff1-42f7-426f-8e41-536af77ce90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10992\n",
            "10992\n",
            "21984\n",
            "21984\n"
          ]
        }
      ],
      "source": [
        "# flip\n",
        "addition = []\n",
        "for i in range(train_size):\n",
        "  addition.append(shuffled_additional_data[i])\n",
        "addition = np.array(addition)\n",
        "training_data = np.concatenate((training_data, addition), axis=0)\n",
        "temp = training_label[:train_size]\n",
        "training_label = np.concatenate((training_label, temp), axis=0)\n",
        "\n",
        "print(len(training_data))\n",
        "print(len(training_label))\n",
        "\n",
        "# # double\n",
        "# temp_d = [sublist*1.5 for sublist in training_data]\n",
        "# for i in range(len(temp_d)):\n",
        "#   training_data.append(temp_d[i])\n",
        "# temp = np.copy(training_label)\n",
        "# training_label = np.concatenate((training_label, temp), axis=0)\n",
        "\n",
        "# noise\n",
        "noise_lower = 0.5\n",
        "noise_upper = 2\n",
        "augmented_data = [sublist + np.random.uniform(noise_lower, noise_upper, size=len(sublist)) for sublist in training_data]\n",
        "for i in range(len(augmented_data)):\n",
        "  if max(augmented_data[i]) != 0:\n",
        "    augmented_data[i] = 255 * augmented_data[i] / max(augmented_data[i])\n",
        "# training_data = np.vstack([training_data, augmented_data])\n",
        "training_data = np.concatenate((training_data, augmented_data), axis=0)\n",
        "temp = np.copy(training_label)\n",
        "training_label = np.concatenate((training_label, temp), axis=0)\n",
        "\n",
        "print(len(training_data))\n",
        "print(len(training_label))\n",
        "# print(training_data[0])\n",
        "\n",
        "# augmented_data = [sublist + np.random.uniform(noise_lower, noise_upper, size=len(sublist)) for sublist in training_data]\n",
        "# for i in range(len(augmented_data)):\n",
        "#   if max(augmented_data[i]) != 0:\n",
        "#     augmented_data[i] = 255 * augmented_data[i] / max(augmented_data[i])\n",
        "# training_data = np.vstack([training_data, augmented_data])\n",
        "# temp = np.copy(training_label)\n",
        "# training_label = np.concatenate((training_label, temp), axis=0)\n",
        "\n",
        "# print(len(training_data))\n",
        "# print(len(training_label))\n",
        "# print(training_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhsVag8Sqrhu"
      },
      "outputs": [],
      "source": [
        "val_label = np.array(val_label)\n",
        "test_label = np.array(test_label)\n",
        "\n",
        "train_data_tensor = [torch.from_numpy(sample) for sample in training_data]\n",
        "train_label_tensor = torch.from_numpy(training_label)\n",
        "val_data_tensor = [torch.from_numpy(sample) for sample in val_data]\n",
        "val_label_tensor = torch.from_numpy(val_label)\n",
        "test_data_tensor = [torch.from_numpy(sample) for sample in test_data]\n",
        "test_label_tensor = torch.from_numpy(test_label)\n",
        "\n",
        "train_set = MyDataset(train_data_tensor, train_label_tensor)\n",
        "val_set = MyDataset(val_data_tensor, val_label_tensor)\n",
        "test_set = MyDataset(test_data_tensor, test_label_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nK0TuXdVv6H"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# construct the three data loader\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W7TNdTQWCEf"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlOLpXlhVx7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc12af7f-1125-4b21-9d36-51cdcfe1240a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:5, train acc:0.8677674672489083, valid acc:0.8529839883551674\n",
            "epoch:10, train acc:0.8771834061135371, valid acc:0.8573508005822417\n",
            "epoch:15, train acc:0.8938318777292577, valid acc:0.8748180494905385\n",
            "epoch:20, train acc:0.8990629548762736, valid acc:0.868995633187773\n",
            "epoch:25, train acc:0.9072052401746725, valid acc:0.8850072780203785\n",
            "epoch:30, train acc:0.9068868267831149, valid acc:0.8748180494905385\n",
            "epoch:35, train acc:0.9120724163027657, valid acc:0.8602620087336245\n",
            "epoch:40, train acc:0.9047943959243085, valid acc:0.8544395924308588\n",
            "epoch:45, train acc:0.9147106986899564, valid acc:0.8544395924308588\n",
            "epoch:50, train acc:0.925900655021834, valid acc:0.8733624454148472\n",
            "epoch:55, train acc:0.9259916302765647, valid acc:0.8850072780203785\n",
            "epoch:60, train acc:0.9338609898107715, valid acc:0.8777292576419214\n",
            "epoch:65, train acc:0.933542576419214, valid acc:0.8777292576419214\n",
            "epoch:70, train acc:0.9382732896652111, valid acc:0.8733624454148472\n",
            "epoch:75, train acc:0.9356804949053857, valid acc:0.8733624454148472\n",
            "epoch:80, train acc:0.9433224163027657, valid acc:0.8704512372634643\n",
            "epoch:85, train acc:0.9419122998544396, valid acc:0.8719068413391557\n",
            "epoch:90, train acc:0.94386826783115, valid acc:0.8719068413391557\n",
            "epoch:95, train acc:0.9506914119359534, valid acc:0.8733624454148472\n",
            "epoch:100, train acc:0.9463700873362445, valid acc:0.8631732168850073\n",
            "epoch:105, train acc:0.9540120087336245, valid acc:0.8704512372634643\n",
            "epoch:110, train acc:0.9460971615720524, valid acc:0.868995633187773\n",
            "epoch:115, train acc:0.951146288209607, valid acc:0.8631732168850073\n",
            "epoch:120, train acc:0.9493267831149927, valid acc:0.8733624454148472\n",
            "epoch:125, train acc:0.9586517467248908, valid acc:0.868995633187773\n",
            "epoch:130, train acc:0.9542394468704513, valid acc:0.8646288209606987\n",
            "epoch:135, train acc:0.957014192139738, valid acc:0.868995633187773\n",
            "epoch:140, train acc:0.9627911208151383, valid acc:0.8646288209606987\n",
            "epoch:145, train acc:0.9632005094614265, valid acc:0.8733624454148472\n",
            "epoch:150, train acc:0.9606986899563319, valid acc:0.8704512372634643\n",
            "epoch:155, train acc:0.9618813682678311, valid acc:0.8733624454148472\n",
            "epoch:160, train acc:0.9710698689956332, valid acc:0.868995633187773\n",
            "epoch:165, train acc:0.963518922852984, valid acc:0.868995633187773\n",
            "epoch:170, train acc:0.9654748908296943, valid acc:0.8500727802037845\n",
            "epoch:175, train acc:0.96365538573508, valid acc:0.8631732168850073\n",
            "epoch:180, train acc:0.9696597525473072, valid acc:0.8704512372634643\n",
            "epoch:185, train acc:0.9659297671033479, valid acc:0.8733624454148472\n",
            "epoch:190, train acc:0.9677492721979621, valid acc:0.8602620087336245\n",
            "epoch:195, train acc:0.9708879184861717, valid acc:0.8675400291120815\n",
            "epoch:200, train acc:0.9702056040756915, valid acc:0.8748180494905385\n",
            "best valid acc: 0.8850072780203785\n"
          ]
        }
      ],
      "source": [
        "model = Model().to(device)\n",
        "modelSaveName = \"Model.pt\"\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "os.chdir(\"/content\")\n",
        "l2_lambda = 0.01\n",
        "\n",
        "bestValidAcc=0\n",
        "for epoch in range(1,epochs+1):\n",
        "    #model.train()\n",
        "    running_loss = 0.0\n",
        "    for step,(x,y) in enumerate(train_loader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        optimizer.zero_grad() # clear the gradient in the previous round\n",
        "        output=model(x) # run the forward pass of the model\n",
        "        loss=lossFunc(output,y) # compute the loss\n",
        "        # l2_regularization_loss = 0\n",
        "        # for param in model.parameters():\n",
        "        #   l2_regularization_loss += torch.norm(param, p=2) ** 2\n",
        "        # loss += 0.5 * l2_lambda * l2_regularization_loss\n",
        "        loss.backward() # compute the gradient of the loss\n",
        "        optimizer.step() # update the weights\n",
        "        running_loss += loss.item()\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    # print(f'Epoch [{epoch}/{epochs}], Loss: {avg_loss}')\n",
        "    if epoch%5==0:\n",
        "        model.eval() # switch the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            correctCount=0\n",
        "            for x,y in train_loader:\n",
        "                x,y=x.to(device),y.to(device)\n",
        "                # print(x.dtype)\n",
        "                pred=model(x).max(1)[1] # argmax\n",
        "                correctCount+=torch.sum(pred==y).item()\n",
        "            trainAcc=correctCount/len(train_loader.dataset)\n",
        "            correctCount=0\n",
        "            for x,y in valid_loader:\n",
        "                x,y=x.to(device),y.to(device)\n",
        "                pred=model(x).max(1)[1]# argmax\n",
        "                correctCount+=torch.sum(pred==y).item()\n",
        "            validAcc=correctCount/len(valid_loader.dataset)\n",
        "        model.train()# switch the model to training mode\n",
        "        print(\"epoch:{}, train acc:{}, valid acc:{}\".format(epoch,trainAcc,validAcc))\n",
        "        if validAcc>bestValidAcc:\n",
        "            bestValidAcc=validAcc\n",
        "            torch.save(model.state_dict(),\"drive/MyDrive/\"+modelSaveName) # save the model to the desired location\n",
        "print(\"best valid acc:\",bestValidAcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAjz8OCCV69o"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bceK6yvaV6Bn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a17e10f-ffd4-4f8a-ce24-c34e82342c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy of the model:  0.8633720930232558\n",
            "Class 1 accuracy: 130 / 145 = 0.896551724137931\n",
            "Class 2 accuracy: 129 / 138 = 0.9347826086956522\n",
            "Class 3 accuracy: 169 / 180 = 0.9388888888888889\n",
            "Class 4 accuracy: 75 / 92 = 0.8152173913043478\n",
            "Class 5 accuracy: 59 / 69 = 0.855072463768116\n",
            "Class 6 accuracy: 32 / 64 = 0.5\n"
          ]
        }
      ],
      "source": [
        "model = Model().to(device)\n",
        "model.load_state_dict(torch.load(\"drive/MyDrive/Model.pt\")) # load the model with the highest valid accuracy\n",
        "model.eval()\n",
        "correctCount=0\n",
        "class_correct = [0 for _ in range(6)]\n",
        "class_total = [0 for _ in range(6)]\n",
        "for x,y in test_loader:\n",
        "    x,y=x.to(device),y.to(device)\n",
        "    pred=model(x).max(1)[1]\n",
        "    correctCount+=torch.sum(pred==y).item()\n",
        "    for i in range(6):\n",
        "        class_correct[i] += torch.sum((pred == i) & (y == i)).item()\n",
        "        class_total[i] += torch.sum(y == i).item()\n",
        "\n",
        "testAcc=correctCount/len(test_loader.dataset)\n",
        "print(\"Testing accuracy of the model: \",testAcc)\n",
        "\n",
        "for i in range(6):\n",
        "    class_acc = class_correct[i] / class_total[i] if class_total[i] != 0 else 0\n",
        "    print(f\"Class {i+1} accuracy: {class_correct[i]} / {class_total[i]} = {class_acc}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}